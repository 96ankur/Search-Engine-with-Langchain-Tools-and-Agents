{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5396e0",
   "metadata": {},
   "source": [
    "## Search Engine With Tools And Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "793aad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "from langchain_classic.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_classic import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68297034",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ab6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the inbuilt tool of wikipedia\n",
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=250)\n",
    "wiki_tool=WikipediaQueryRun(api_wrapper=api_wrapper_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63eca269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the inbuilt tool of arxiv\n",
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=250)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=api_wrapper_arxiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01dd0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tool [RAG tool]\n",
    "loader = WebBaseLoader(\"https://docs.langchain.com/langsmith/home\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "vectordb = FAISS.from_documents(documents,HuggingFaceEmbeddings())\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "## create retriever as tool\n",
    "retriever_tool = create_retriever_tool(retriever, \"langsmith=search\", \"search any information about Langsmith\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "864c64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki_tool, arxiv_tool, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2aa63f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run all this tools with Agents and LLM Models\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965bc51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt Template\n",
    "# An alternate way for prompt template\n",
    "prompt = hub.pull('hwchase17/openai-functions-agent')\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a07edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001854E9EADE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001854E9EADE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x00000185439DCEC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000185439DD550>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith=search', 'description': 'search any information about Langsmith', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "agent = create_openai_tools_agent(llm,tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4f97757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Executer\n",
    "agent_executor = AgentExecutor(agent=agent,tools=tools,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0186d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith=search` with `{'query': 'Langsmith information'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mLangSmith docs - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith docsGet startedObservabilityEvaluationPrompt engineeringDeploymentAgent BuilderPlatform setupReferenceOverviewPlansCreate an account and API keyAccount administrationOverviewSet up hierarchyManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesPolly (Beta)Data managementAccess control & AuthenticationScalability & resilienceFAQsRegions FAQPricing FAQLangSmith statusLangSmith docsCopy pageCopy pageLangSmith provides tools for developing, debugging, and deploying LLM applications.\n",
      "It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.\n",
      "LangSmith is framework agnostic, so you can use it with or without LangChain’s open-source libraries\n",
      "langchain and langgraph.\n",
      "\n",
      "LangSmith is framework agnostic, so you can use it with or without LangChain’s open-source libraries\n",
      "langchain and langgraph.\n",
      "Prototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\n",
      "LangGraph Platform is now LangSmith Deployment. For more information, check out the Changelog.\n",
      "​Get started\n",
      "Create an accountSign up at smith.langchain.com (no credit card required).\n",
      "You can log in with Google, GitHub, or email.Create an API keyGo to your Settings page → API Keys → Create API Key.\n",
      "Copy the key and save it securely.\n",
      "Once your account and API key are ready, choose a quickstart to begin building with LangSmith:\n",
      "\n",
      "​More ways to build\n",
      "Platform setupUse LangSmith in managed cloud, in a self-hosted environment, or hybrid to match your infrastructure and compliance needs.Choose how to set up LangSmithAgent Builder (Beta)Design and deploy AI agents visually with a no-code interface—perfect for rapid prototyping and getting started without writing code.Build an agentStudioUse a visual interface to design, test, and refine applications end-to-end.Develop with StudioPrompt testingIterate on prompts with built-in versioning and collaboration to ship improvements faster.Test your prompts\n",
      "LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.\n",
      "​Workflow\n",
      "LangSmith combines observability, evaluation, deployment, and platform setup in one integrated workflow—from local development to production.\n",
      "\n",
      "Edit this page on GitHub or file an issue.\n",
      "Connect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPricing plansNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform that provides tools for developing, debugging, and deploying Large Language Model (LLM) applications. It offers features such as observability, evaluation, prompt testing, and deployment, and is framework agnostic, allowing users to use it with or without LangChain's open-source libraries. LangSmith provides a visual interface for designing and deploying AI agents, as well as a no-code interface for rapid prototyping. It also meets high standards of data security and privacy, with HIPAA, SOC 2 Type 2, and GDPR compliance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith',\n",
       " 'output': \"LangSmith is a platform that provides tools for developing, debugging, and deploying Large Language Model (LLM) applications. It offers features such as observability, evaluation, prompt testing, and deployment, and is framework agnostic, allowing users to use it with or without LangChain's open-source libraries. LangSmith provides a visual interface for designing and deploying AI agents, as well as a no-code interface for rapid prototyping. It also meets high standards of data security and privacy, with HIPAA, SOC 2 Type 2, and GDPR compliance.\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Tell me about Langsmith\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41376655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is machine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_classic\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    166\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    172\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    173\u001b[39m         inputs,\n\u001b[32m    174\u001b[39m         outputs,\n\u001b[32m    175\u001b[39m         return_only_outputs,\n\u001b[32m    176\u001b[39m     )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_classic\\agents\\agent.py:1590\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1588\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1590\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1597\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1598\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1599\u001b[39m             next_step_output,\n\u001b[32m   1600\u001b[39m             intermediate_steps,\n\u001b[32m   1601\u001b[39m             run_manager=run_manager,\n\u001b[32m   1602\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_classic\\agents\\agent.py:1290\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1281\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1282\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1283\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1287\u001b[39m     run_manager: CallbackManagerForChainRun | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1288\u001b[39m ) -> AgentFinish | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m   1289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m         \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1293\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1294\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1295\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1299\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_classic\\agents\\agent.py:1317\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1314\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1316\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1317\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_classic\\agents\\agent.py:557\u001b[39m, in \u001b[36mRunnableMultiActionAgent.plan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    549\u001b[39m final_output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_runnable:\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3528\u001b[39m, in \u001b[36mRunnableSequence.stream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3521\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream\u001b[39m(\n\u001b[32m   3523\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3526\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3527\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3528\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3514\u001b[39m, in \u001b[36mRunnableSequence.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3507\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m   3509\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3512\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3513\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3514\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_stream_with_config(\n\u001b[32m   3515\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3516\u001b[39m         \u001b[38;5;28mself\u001b[39m._transform,\n\u001b[32m   3517\u001b[39m         patch_config(config, run_name=(config \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name),\n\u001b[32m   3518\u001b[39m         **kwargs,\n\u001b[32m   3519\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2324\u001b[39m, in \u001b[36mRunnable._transform_stream_with_config\u001b[39m\u001b[34m(self, inputs, transformer, config, run_type, **kwargs)\u001b[39m\n\u001b[32m   2322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2323\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2324\u001b[39m         chunk: Output = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2325\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m   2326\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3480\u001b[39m, in \u001b[36mRunnableSequence._transform\u001b[39m\u001b[34m(self, inputs, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   3477\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3478\u001b[39m         final_pipeline = step.transform(final_pipeline, config)\n\u001b[32m-> \u001b[39m\u001b[32m3480\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1542\u001b[39m, in \u001b[36mRunnable.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1539\u001b[39m final: Input\n\u001b[32m   1540\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1542\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The default implementation of transform is to buffer input and\u001b[39;49;00m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# then call stream.\u001b[39;49;00m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# It'll attempt to gather all input into a single chunk using\u001b[39;49;00m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the `+` operator.\u001b[39;49;00m\n\u001b[32m   1547\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If the input is not addable, then we'll assume that we can\u001b[39;49;00m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# only operate on the last chunk,\u001b[39;49;00m\n\u001b[32m   1549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# and we'll iterate until we get to the last chunk.\u001b[39;49;00m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgot_first_val\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5773\u001b[39m, in \u001b[36mRunnableBindingBase.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5766\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5767\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m   5768\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5771\u001b[39m     **kwargs: Any,\n\u001b[32m   5772\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m5773\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.transform(\n\u001b[32m   5774\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5775\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5776\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5777\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1560\u001b[39m, in \u001b[36mRunnable.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m             final = ichunk\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(final, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:536\u001b[39m, in \u001b[36mBaseChatModel.stream\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    534\u001b[39m index = -\u001b[32m1\u001b[39m\n\u001b[32m    535\u001b[39m index_type = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:629\u001b[39m, in \u001b[36mChatGroq._stream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m params = {**params, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m    628\u001b[39m default_chunk_class: \u001b[38;5;28mtype\u001b[39m[BaseMessageChunk] = AIMessageChunk\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: PLW2901\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\groq\\_streaming.py:46\u001b[39m, in \u001b[36mStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[_T]:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\GenerativeAI\\GenAI\\Search Engine with Langchain Tools and Agents\\.venv\\Lib\\site-packages\\groq\\_streaming.py:91\u001b[39m, in \u001b[36mStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     89\u001b[39m                 message = \u001b[33m\"\u001b[39m\u001b[33mAn error occurred during streaming\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[32m     92\u001b[39m                 message=message,\n\u001b[32m     93\u001b[39m                 request=\u001b[38;5;28mself\u001b[39m.response.request,\n\u001b[32m     94\u001b[39m                 body=data[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     95\u001b[39m             )\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m process_data(data={\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: data, \u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m: sse.event}, cast_to=cast_to, response=response)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# The stream needs to be fully consumed to close the response\u001b[39;00m\n",
      "\u001b[31mAPIError\u001b[39m: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details."
     ]
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is machine learning\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Search Engine with Langchain Tools and Agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
